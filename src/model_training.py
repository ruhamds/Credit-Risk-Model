# -*- coding: utf-8 -*-
"""task5_model_training.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/16sNeA7sqFN9JmORDQx7qBt43y9zDR864
"""

import pandas as pd
import numpy as np
import mlflow
import mlflow.sklearn
from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier
from sklearn.linear_model import LogisticRegression
from sklearn.tree import DecisionTreeClassifier
from sklearn.model_selection import GridSearchCV, RandomizedSearchCV
from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score
from sklearn.metrics import classification_report, confusion_matrix
from imblearn.over_sampling import SMOTE
import matplotlib.pyplot as plt
import seaborn as sns
import joblib
import warnings
warnings.filterwarnings('ignore')

# Commented out IPython magic to ensure Python compatibility.
# %pip install mlflow

# Load preprocessed data
X_train = pd.read_csv('features_train_woe.csv')
X_test = pd.read_csv('features_test_woe.csv')
y_train = pd.read_csv('target_train.csv').iloc[:, 0]
y_test = pd.read_csv('target_test.csv').iloc[:, 0]

print(f"Training data shape: {X_train.shape}")
print(f"Test data shape: {X_test.shape}")
print(f"Class distribution in training:\n{y_train.value_counts()}")

"""1. SMOTE for Class Balancing"""

# Apply SMOTE to balance the dataset
smote = SMOTE(random_state=42)
X_train_balanced, y_train_balanced = smote.fit_resample(X_train, y_train)

print(f"Original training shape: {X_train.shape}")
print(f"Balanced training shape: {X_train_balanced.shape}")
print(f"Balanced class distribution:\n{pd.Series(y_train_balanced).value_counts()}")

"""2. MLflow Setup and Model Training"""

import mlflow
import mlflow.sklearn
from mlflow.models.signature import infer_signature

# Example: assuming `X_train` and `y_train` used for training
# signature = infer_signature(X_train, best_model.predict(X_train))
# input_example = X_train.iloc[:5]  # or use .head()

# mlflow.sklearn.log_model(
#     best_model,
#     model_name,
#     registered_model_name=model_name,
#     signature=signature,
#     input_example=input_example
# )

# Initialize MLflow
mlflow.set_experiment("Customer_Risk_Prediction")

def evaluate_model(model, X_test, y_test, y_pred, y_pred_proba):
    """Calculate all evaluation metrics"""
    metrics = {
        'accuracy': accuracy_score(y_test, y_pred),
        'precision': precision_score(y_test, y_pred),
        'recall': recall_score(y_test, y_pred),
        'f1_score': f1_score(y_test, y_pred),
        'roc_auc': roc_auc_score(y_test, y_pred_proba)
    }
    return metrics

def train_and_log_model(model_name, model, X_train, y_train, X_test, y_test, params=None):
    """Train model and log to MLflow"""
    with mlflow.start_run(run_name=model_name):
        # Train model
        model.fit(X_train, y_train)

        # Predictions
        y_pred = model.predict(X_test)
        y_pred_proba = model.predict_proba(X_test)[:, 1]

        # Calculate metrics
        metrics = evaluate_model(model, X_test, y_test, y_pred, y_pred_proba)

        # Log parameters
        if params:
            mlflow.log_params(params)

        # Log metrics
        mlflow.log_metrics(metrics)

        # Log model
        mlflow.sklearn.log_model(model, f"{model_name}_model")

        print(f"\n{model_name} Results:")
        for metric, value in metrics.items():
            print(f"{metric}: {value:.4f}")

        return model, metrics

# Model configurations
models_config = {
    'Logistic_Regression': LogisticRegression(random_state=42, max_iter=1000),
    'Decision_Tree': DecisionTreeClassifier(random_state=42),
    'Random_Forest': RandomForestClassifier(random_state=42),
    'Gradient_Boosting': GradientBoostingClassifier(random_state=42)
}

# Train baseline models
baseline_results = {}
for name, model in models_config.items():
    trained_model, metrics = train_and_log_model(
        f"{name}_Baseline", model, X_train_balanced, y_train_balanced, X_test, y_test
    )
    baseline_results[name] = metrics

"""3. Hyperparameter Tuning"""

# Hyperparameter grids
param_grids = {
    'Random_Forest': {
        'n_estimators': [100, 200],
        'max_depth': [10, 20, None],
        'min_samples_split': [2, 5],
        'min_samples_leaf': [1, 2]
    },
    'Gradient_Boosting': {
        'n_estimators': [100, 200],
        'learning_rate': [0.05, 0.1, 0.15],
        'max_depth': [3, 5, 7]
    },
    'Logistic_Regression': {
        'C': [0.1, 1, 10],
        'penalty': ['l1', 'l2'],
        'solver': ['liblinear']
    }
}

tuned_results = {}

for model_name in ['Random_Forest', 'Gradient_Boosting', 'Logistic_Regression']:
    print(f"\nTuning {model_name}...")

    with mlflow.start_run(run_name=f"{model_name}_Tuned"):
        # Grid search
        grid_search = GridSearchCV(
            models_config[model_name],
            param_grids[model_name],
            cv=3,
            scoring='roc_auc',
            n_jobs=-1
        )

        grid_search.fit(X_train_balanced, y_train_balanced)

        # Best model predictions
        best_model = grid_search.best_estimator_
        y_pred = best_model.predict(X_test)
        y_pred_proba = best_model.predict_proba(X_test)[:, 1]

        # Calculate metrics
        metrics = evaluate_model(best_model, X_test, y_test, y_pred, y_pred_proba)

        # Log best parameters and metrics
        mlflow.log_params(grid_search.best_params_)
        mlflow.log_metrics(metrics)
        mlflow.sklearn.log_model(best_model, f"{model_name}_tuned_model")

        tuned_results[model_name] = {
            'model': best_model,
            'params': grid_search.best_params_,
            'metrics': metrics
        }

        print(f"Best params: {grid_search.best_params_}")
        print(f"Best ROC-AUC: {metrics['roc_auc']:.4f}")

"""4. Model Comparison and Best Model Selection"""

# Compare all models
comparison_df = pd.DataFrame({
    name: metrics for name, result in tuned_results.items()
    for metrics in [result['metrics']]
}).T

print("Model Comparison (Tuned Models):")
print(comparison_df.round(4))

# Visualize model comparison
fig, axes = plt.subplots(2, 3, figsize=(15, 10))
metrics_to_plot = ['accuracy', 'precision', 'recall', 'f1_score', 'roc_auc']

for i, metric in enumerate(metrics_to_plot):
    ax = axes[i//3, i%3]
    comparison_df[metric].plot(kind='bar', ax=ax)
    ax.set_title(f'{metric.upper()}')
    ax.set_ylabel('Score')
    plt.setp(ax.get_xticklabels(), rotation=45)

plt.tight_layout()
plt.show()

# Select best model based on ROC-AUC
best_model_name = comparison_df['roc_auc'].idxmax()
best_model = tuned_results[best_model_name]['model']

print(f"\nBest Model: {best_model_name}")
print(f"Best ROC-AUC: {comparison_df.loc[best_model_name, 'roc_auc']:.4f}")

"""5. Register Best Model in MLflow"""

# Register the best model
with mlflow.start_run(run_name=f"Best_Model_{best_model_name}"):
    # Final predictions with best model
    y_pred_final = best_model.predict(X_test)
    y_pred_proba_final = best_model.predict_proba(X_test)[:, 1]

    # Final metrics
    final_metrics = evaluate_model(best_model, X_test, y_test, y_pred_final, y_pred_proba_final)

    # Log everything
    mlflow.log_params(tuned_results[best_model_name]['params'])
    mlflow.log_metrics(final_metrics)

    # Log model
    model_info = mlflow.sklearn.log_model(
        best_model,
        "best_customer_risk_model",
        registered_model_name="CustomerRiskModel"
    )

    # Save locally
    joblib.dump(best_model, 'best_customer_risk_model.pkl')

    print("Best model registered and saved!")
    print(f"Model URI: {model_info.model_uri}")

# Detailed evaluation of best model
print(f"\nDetailed Evaluation - {best_model_name}:")
print("Classification Report:")
print(classification_report(y_test, y_pred_final))

# Confusion Matrix
plt.figure(figsize=(8, 6))
cm = confusion_matrix(y_test, y_pred_final)
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.title(f'Confusion Matrix - {best_model_name}')
plt.ylabel('Actual')
plt.xlabel('Predicted')
plt.show()

"""6. Feature Importance Analysis"""

# Feature importance (for tree-based models)
if hasattr(best_model, 'feature_importances_'):
    feature_importance = pd.DataFrame({
        'feature': X_train.columns,
        'importance': best_model.feature_importances_
    }).sort_values('importance', ascending=False)

    plt.figure(figsize=(10, 6))
    plt.barh(feature_importance['feature'], feature_importance['importance'])
    plt.title(f'Feature Importance - {best_model_name}')
    plt.xlabel('Importance')
    plt.tight_layout()
    plt.show()

    print("Top Features:")
    print(feature_importance.head())

print("\nModel training pipeline completed successfully!")
print(f"Best model: {best_model_name} with ROC-AUC: {final_metrics['roc_auc']:.4f}")

import matplotlib.pyplot as plt
from sklearn.metrics import roc_curve, confusion_matrix, ConfusionMatrixDisplay

# ROC Curve
fpr, tpr, _ = roc_curve(y_test, best_model.predict_proba(X_test)[:,1])
plt.plot(fpr, tpr, label='ROC curve')
plt.plot([0, 1], [0, 1], 'k--')
plt.xlabel('False Positive Rate')
plt.ylabel('True Positive Rate')
plt.title('ROC Curve')
plt.legend()
plt.grid()
plt.show()
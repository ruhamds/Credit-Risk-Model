{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "SfLxtMiv7FLN"
      },
      "outputs": [],
      "source": [
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import sys\n",
        "import os\n",
        "\n",
        "# Add current directory to path for imports\n",
        "sys.path.append(os.getcwd())"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Helper functions that we'll test\n",
        "def calculate_rfm_metrics(df, customer_id='CustomerId', order_date='InvoiceDate', amount='Amount'):\n",
        "    \"\"\"Calculate RFM metrics for customers\"\"\"\n",
        "    # Convert date column\n",
        "    df[order_date] = pd.to_datetime(df[order_date])\n",
        "    current_date = df[order_date].max()\n",
        "\n",
        "    rfm = df.groupby(customer_id).agg({\n",
        "        order_date: lambda x: (current_date - x.max()).days,  # Recency\n",
        "        'InvoiceNo': 'count',  # Frequency\n",
        "        amount: 'sum'  # Monetary\n",
        "    }).reset_index()\n",
        "\n",
        "    rfm.columns = [customer_id, 'recency', 'frequency', 'monetary']\n",
        "    return rfm\n",
        "\n",
        "def calculate_iv_score(df, feature_col, target_col):\n",
        "    \"\"\"Calculate Information Value for a feature\"\"\"\n",
        "    if df[feature_col].dtype in ['int64', 'float64']:\n",
        "        # Create bins for continuous variables\n",
        "        df_temp = df.copy()\n",
        "        df_temp['feature_binned'] = pd.qcut(df_temp[feature_col], q=5, duplicates='drop')\n",
        "        feature_col = 'feature_binned'\n",
        "        df = df_temp\n",
        "\n",
        "    # Create crosstab\n",
        "    crosstab = pd.crosstab(df[feature_col], df[target_col])\n",
        "\n",
        "    if crosstab.shape[1] != 2:\n",
        "        return 0.0\n",
        "\n",
        "    crosstab['total'] = crosstab.sum(axis=1)\n",
        "    crosstab['good_rate'] = crosstab.iloc[:, 0] / crosstab.iloc[:, 0].sum()\n",
        "    crosstab['bad_rate'] = crosstab.iloc[:, 1] / crosstab.iloc[:, 1].sum()\n",
        "\n",
        "    # Avoid division by zero\n",
        "    crosstab['good_rate'] = crosstab['good_rate'].replace(0, 0.0001)\n",
        "    crosstab['bad_rate'] = crosstab['bad_rate'].replace(0, 0.0001)\n",
        "\n",
        "    # Calculate WOE and IV\n",
        "    crosstab['woe'] = np.log(crosstab['good_rate'] / crosstab['bad_rate'])\n",
        "    crosstab['iv'] = (crosstab['good_rate'] - crosstab['bad_rate']) * crosstab['woe']\n",
        "\n",
        "    return crosstab['iv'].sum()\n",
        "\n",
        "def validate_model_metrics(y_true, y_pred, y_pred_proba=None):\n",
        "    \"\"\"Validate that model metrics are within expected ranges\"\"\"\n",
        "    from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
        "\n",
        "    metrics = {}\n",
        "    metrics['accuracy'] = accuracy_score(y_true, y_pred)\n",
        "    metrics['precision'] = precision_score(y_true, y_pred)\n",
        "    metrics['recall'] = recall_score(y_true, y_pred)\n",
        "    metrics['f1_score'] = f1_score(y_true, y_pred)\n",
        "\n",
        "    if y_pred_proba is not None:\n",
        "        metrics['roc_auc'] = roc_auc_score(y_true, y_pred_proba)\n",
        "\n",
        "    # Validate ranges\n",
        "    for metric, value in metrics.items():\n",
        "        if not (0 <= value <= 1):\n",
        "            raise ValueError(f\"{metric} is out of range [0,1]: {value}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "print(\"Helper functions defined successfully!\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EQ0LQHr79mxa",
        "outputId": "0318dcd1-9f85-42c4-aeb2-11f8c826d58f"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Helper functions defined successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Unit Tests Implementation"
      ],
      "metadata": {
        "id": "VDuhdQxA_BrT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test cases\n",
        "class TestDataProcessing:\n",
        "\n",
        "    def test_calculate_rfm_metrics(self):\n",
        "        \"\"\"Test RFM calculation function\"\"\"\n",
        "        # Create sample data\n",
        "        test_data = pd.DataFrame({\n",
        "            'CustomerId': [1, 1, 2, 2, 3],\n",
        "            'InvoiceDate': ['2023-01-01', '2023-01-15', '2023-01-10', '2023-01-20', '2023-01-05'],\n",
        "            'InvoiceNo': ['A001', 'A002', 'A003', 'A004', 'A005'],\n",
        "            'Amount': [100, 150, 200, 80, 300]\n",
        "        })\n",
        "\n",
        "        # Calculate RFM\n",
        "        rfm_result = calculate_rfm_metrics(test_data)\n",
        "\n",
        "        # Assertions\n",
        "        assert len(rfm_result) == 3, \"Should have 3 unique customers\"\n",
        "        assert 'recency' in rfm_result.columns, \"Should have recency column\"\n",
        "        assert 'frequency' in rfm_result.columns, \"Should have frequency column\"\n",
        "        assert 'monetary' in rfm_result.columns, \"Should have monetary column\"\n",
        "        assert rfm_result['frequency'].sum() == 5, \"Total frequency should equal number of transactions\"\n",
        "        assert rfm_result['monetary'].sum() == 830, \"Total monetary should equal sum of amounts\"\n",
        "\n",
        "        print(\"âœ“ test_calculate_rfm_metrics passed\")\n",
        "\n",
        "    def test_calculate_iv_score(self):\n",
        "        \"\"\"Test Information Value calculation\"\"\"\n",
        "        # Create sample data with known pattern\n",
        "        np.random.seed(42)\n",
        "        test_data = pd.DataFrame({\n",
        "            'feature': np.random.normal(0, 1, 1000),\n",
        "            'target': np.random.binomial(1, 0.3, 1000)\n",
        "        })\n",
        "\n",
        "        # Add some correlation\n",
        "        test_data.loc[test_data['target'] == 1, 'feature'] += 0.5\n",
        "\n",
        "        # Calculate IV\n",
        "        iv_score = calculate_iv_score(test_data, 'feature', 'target')\n",
        "\n",
        "        # Assertions\n",
        "        assert isinstance(iv_score, float), \"IV score should be a float\"\n",
        "        assert iv_score >= 0, \"IV score should be non-negative\"\n",
        "        assert iv_score < 10, \"IV score should be reasonable (< 10)\"\n",
        "\n",
        "        print(f\"âœ“ test_calculate_iv_score passed (IV: {iv_score:.4f})\")\n",
        "\n",
        "    def test_validate_model_metrics(self):\n",
        "        \"\"\"Test model metrics validation\"\"\"\n",
        "        # Create sample predictions\n",
        "        y_true = np.array([0, 0, 1, 1, 0, 1, 1, 0])\n",
        "        y_pred = np.array([0, 1, 1, 1, 0, 0, 1, 0])\n",
        "        y_pred_proba = np.array([0.2, 0.6, 0.8, 0.9, 0.3, 0.4, 0.7, 0.1])\n",
        "\n",
        "        # Calculate metrics\n",
        "        metrics = validate_model_metrics(y_true, y_pred, y_pred_proba)\n",
        "\n",
        "        # Assertions\n",
        "        assert len(metrics) == 5, \"Should return 5 metrics\"\n",
        "        assert all(0 <= v <= 1 for v in metrics.values()), \"All metrics should be between 0 and 1\"\n",
        "        assert 'accuracy' in metrics, \"Should include accuracy\"\n",
        "        assert 'precision' in metrics, \"Should include precision\"\n",
        "        assert 'recall' in metrics, \"Should include recall\"\n",
        "        assert 'f1_score' in metrics, \"Should include f1_score\"\n",
        "        assert 'roc_auc' in metrics, \"Should include roc_auc\"\n",
        "\n",
        "        print(\"âœ“ test_validate_model_metrics passed\")\n",
        "        print(f\"  Metrics: {metrics}\")\n",
        "\n",
        "# Run the tests\n",
        "def run_tests():\n",
        "    \"\"\"Run all unit tests\"\"\"\n",
        "    test_suite = TestDataProcessing()\n",
        "\n",
        "    try:\n",
        "        test_suite.test_calculate_rfm_metrics()\n",
        "        test_suite.test_calculate_iv_score()\n",
        "        test_suite.test_validate_model_metrics()\n",
        "        print(\"\\nðŸŽ‰ All tests passed successfully!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Test failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Execute tests\n",
        "test_results = run_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0jCvcUrT-_Y4",
        "outputId": "06289ddf-0f8a-4d73-8eff-852dcc503fc1"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ test_calculate_rfm_metrics passed\n",
            "âœ“ test_calculate_iv_score passed (IV: 0.3630)\n",
            "âœ“ test_validate_model_metrics passed\n",
            "  Metrics: {'accuracy': 0.75, 'precision': 0.75, 'recall': 0.75, 'f1_score': 0.75, 'roc_auc': np.float64(0.9375)}\n",
            "\n",
            "ðŸŽ‰ All tests passed successfully!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Advanced Test Cases"
      ],
      "metadata": {
        "id": "FnYC1Ru2_GlT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class TestEdgeCases:\n",
        "\n",
        "    def test_empty_dataframe(self):\n",
        "        \"\"\"Test handling of empty dataframes\"\"\"\n",
        "        empty_df = pd.DataFrame()\n",
        "\n",
        "        try:\n",
        "            # This should handle gracefully or raise appropriate error\n",
        "            result = calculate_rfm_metrics(empty_df)\n",
        "            assert len(result) == 0, \"Empty dataframe should return empty result\"\n",
        "            print(\"âœ“ test_empty_dataframe passed\")\n",
        "        except Exception as e:\n",
        "            print(f\"âœ“ test_empty_dataframe passed (expected error: {type(e).__name__})\")\n",
        "\n",
        "    def test_single_customer(self):\n",
        "        \"\"\"Test RFM calculation with single customer\"\"\"\n",
        "        single_customer_data = pd.DataFrame({\n",
        "            'CustomerId': [1],\n",
        "            'InvoiceDate': ['2023-01-01'],\n",
        "            'InvoiceNo': ['A001'],\n",
        "            'Amount': [100]\n",
        "        })\n",
        "\n",
        "        rfm_result = calculate_rfm_metrics(single_customer_data)\n",
        "\n",
        "        assert len(rfm_result) == 1, \"Should have 1 customer\"\n",
        "        assert rfm_result['frequency'].iloc[0] == 1, \"Frequency should be 1\"\n",
        "        assert rfm_result['monetary'].iloc[0] == 100, \"Monetary should be 100\"\n",
        "\n",
        "        print(\"âœ“ test_single_customer passed\")\n",
        "\n",
        "    def test_iv_with_no_variance(self):\n",
        "        \"\"\"Test IV calculation with constant target\"\"\"\n",
        "        constant_target_data = pd.DataFrame({\n",
        "            'feature': [1, 2, 3, 4, 5],\n",
        "            'target': [0, 0, 0, 0, 0]  # No variance\n",
        "        })\n",
        "\n",
        "        iv_score = calculate_iv_score(constant_target_data, 'feature', 'target')\n",
        "\n",
        "        # IV should be 0 or very low for constant target\n",
        "        assert iv_score >= 0, \"IV should be non-negative even with no target variance\"\n",
        "\n",
        "        print(f\"âœ“ test_iv_with_no_variance passed (IV: {iv_score:.4f})\")\n",
        "\n",
        "# Run advanced tests\n",
        "def run_advanced_tests():\n",
        "    \"\"\"Run edge case tests\"\"\"\n",
        "    advanced_suite = TestEdgeCases()\n",
        "\n",
        "    try:\n",
        "        advanced_suite.test_empty_dataframe()\n",
        "        advanced_suite.test_single_customer()\n",
        "        advanced_suite.test_iv_with_no_variance()\n",
        "        print(\"\\nðŸš€ All advanced tests passed!\")\n",
        "        return True\n",
        "    except Exception as e:\n",
        "        print(f\"\\nâŒ Advanced test failed: {str(e)}\")\n",
        "        return False\n",
        "\n",
        "# Execute advanced tests\n",
        "advanced_results = run_advanced_tests()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "RJyi43Xq_KT-",
        "outputId": "7206e244-6984-49f4-f8db-745c115e7308"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ“ test_empty_dataframe passed (expected error: KeyError)\n",
            "âœ“ test_single_customer passed\n",
            "âœ“ test_iv_with_no_variance passed (IV: 0.0000)\n",
            "\n",
            "ðŸš€ All advanced tests passed!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Test Summary and Results"
      ],
      "metadata": {
        "id": "u7ZJB3IH_QOI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create test results summary\n",
        "def create_test_summary():\n",
        "    \"\"\"Create a summary of all test results\"\"\"\n",
        "\n",
        "    summary = {\n",
        "        'Total Tests': 6,\n",
        "        'Basic Tests': 3,\n",
        "        'Advanced Tests': 3,\n",
        "        'Basic Tests Passed': test_results,\n",
        "        'Advanced Tests Passed': advanced_results,\n",
        "        'Overall Success': test_results and advanced_results\n",
        "    }\n",
        "\n",
        "    print(\"\\n\" + \"=\"*50)\n",
        "    print(\"UNIT TEST SUMMARY\")\n",
        "    print(\"=\"*50)\n",
        "\n",
        "    for key, value in summary.items():\n",
        "        print(f\"{key}: {value}\")\n",
        "\n",
        "    if summary['Overall Success']:\n",
        "        print(\"\\nâœ… ALL TESTS PASSED - Code is ready for production!\")\n",
        "    else:\n",
        "        print(\"\\nâš ï¸  Some tests failed - Review code before deployment\")\n",
        "\n",
        "    return summary\n",
        "\n",
        "# Generate final summary\n",
        "test_summary = create_test_summary()\n",
        "\n",
        "# Save test results to file (for CI/CD integration)\n",
        "import json\n",
        "\n",
        "with open('test_results.json', 'w') as f:\n",
        "    json.dump({\n",
        "        'timestamp': pd.Timestamp.now().isoformat(),\n",
        "        'results': test_summary,\n",
        "        'status': 'PASSED' if test_summary['Overall Success'] else 'FAILED'\n",
        "    }, f, indent=2)\n",
        "\n",
        "print(\"\\nTest results saved to 'test_results.json'\")\n",
        "print(\"\\nUnit testing completed! âœ¨\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ue-Xyhe5_ULr",
        "outputId": "8683fa50-da42-4c24-81c9-331767ceb06b"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "==================================================\n",
            "UNIT TEST SUMMARY\n",
            "==================================================\n",
            "Total Tests: 6\n",
            "Basic Tests: 3\n",
            "Advanced Tests: 3\n",
            "Basic Tests Passed: True\n",
            "Advanced Tests Passed: True\n",
            "Overall Success: True\n",
            "\n",
            "âœ… ALL TESTS PASSED - Code is ready for production!\n",
            "\n",
            "Test results saved to 'test_results.json'\n",
            "\n",
            "Unit testing completed! âœ¨\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Integration with pytest (Optional)"
      ],
      "metadata": {
        "id": "kGE-06MP_YEv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Save this as tests/test_data_processing.py for pytest integration\n",
        "test_file_content = '''\n",
        "import pytest\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from your_module import calculate_rfm_metrics, calculate_iv_score, validate_model_metrics\n",
        "\n",
        "class TestDataProcessing:\n",
        "\n",
        "    def test_calculate_rfm_metrics(self):\n",
        "        \"\"\"Test RFM calculation function\"\"\"\n",
        "        test_data = pd.DataFrame({\n",
        "            'CustomerId': [1, 1, 2, 2, 3],\n",
        "            'InvoiceDate': ['2023-01-01', '2023-01-15', '2023-01-10', '2023-01-20', '2023-01-05'],\n",
        "            'InvoiceNo': ['A001', 'A002', 'A003', 'A004', 'A005'],\n",
        "            'Amount': [100, 150, 200, 80, 300]\n",
        "        })\n",
        "\n",
        "        rfm_result = calculate_rfm_metrics(test_data)\n",
        "\n",
        "        assert len(rfm_result) == 3\n",
        "        assert 'recency' in rfm_result.columns\n",
        "        assert 'frequency' in rfm_result.columns\n",
        "        assert 'monetary' in rfm_result.columns\n",
        "\n",
        "    def test_calculate_iv_score(self):\n",
        "        \"\"\"Test Information Value calculation\"\"\"\n",
        "        np.random.seed(42)\n",
        "        test_data = pd.DataFrame({\n",
        "            'feature': np.random.normal(0, 1, 1000),\n",
        "            'target': np.random.binomial(1, 0.3, 1000)\n",
        "        })\n",
        "\n",
        "        iv_score = calculate_iv_score(test_data, 'feature', 'target')\n",
        "\n",
        "        assert isinstance(iv_score, float)\n",
        "        assert iv_score >= 0\n",
        "'''\n",
        "\n",
        "# Create the directory if it doesn't exist\n",
        "if not os.path.exists('tests'):\n",
        "    os.makedirs('tests')\n",
        "\n",
        "# Write to file for pytest usage\n",
        "with open('tests/test_data_processing.py', 'w') as f:\n",
        "    f.write(test_file_content)\n",
        "\n",
        "print(\"Pytest integration file created at 'tests/test_data_processing.py'\")\n",
        "print(\"Run with: pytest tests/test_data_processing.py -v\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A_fikXny_Xmt",
        "outputId": "fe5af88d-772c-44ed-c339-2a27db16ed47"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pytest integration file created at 'tests/test_data_processing.py'\n",
            "Run with: pytest tests/test_data_processing.py -v\n"
          ]
        }
      ]
    }
  ]
}
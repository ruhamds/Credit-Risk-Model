{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "1. Setup and Imports"
      ],
      "metadata": {
        "id": "X4JdnVyQ65vp"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "rxep9jespDEF"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.pipeline import Pipeline, FeatureUnion\n",
        "from sklearn.compose import ColumnTransformer\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, OneHotEncoder, LabelEncoder,OrdinalEncoder\n",
        "from sklearn.impute import SimpleImputer, KNNImputer\n",
        "from sklearn.base import BaseEstimator, TransformerMixin\n",
        "import joblib\n",
        "import sys\n",
        "import os"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Add src to path\n",
        "sys.path.append('src/')"
      ],
      "metadata": {
        "id": "0U40AdTq7C3k"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Load and Clean Data"
      ],
      "metadata": {
        "id": "owUR2scd8THr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Load the cleaned data from Task 2\n",
        "df = pd.read_csv('data.csv')\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "\n",
        "# Drop useless columns with no variance\n",
        "columns_to_drop = ['BatchID', 'CountryCode', 'CurrencyCode']\n",
        "existing_drops = [col for col in columns_to_drop if col in df.columns]\n",
        "if existing_drops:\n",
        "    df = df.drop(columns=existing_drops)\n",
        "    print(f\"Dropped columns: {existing_drops}\")\n",
        "\n",
        "print(f\"Final shape: {df.shape}\")\n",
        "print(f\"Columns: {df.columns.tolist()}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K6RlBbNR8LME",
        "outputId": "9dd42647-8ba2-429f-e8f0-bf3d7ee4b3fb"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Original shape: (95662, 16)\n",
            "Dropped columns: ['CountryCode', 'CurrencyCode']\n",
            "Final shape: (95662, 14)\n",
            "Columns: ['TransactionId', 'BatchId', 'AccountId', 'SubscriptionId', 'CustomerId', 'ProviderId', 'ProductId', 'ProductCategory', 'ChannelId', 'Amount', 'Value', 'TransactionStartTime', 'PricingStrategy', 'FraudResult']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Custom Transformers"
      ],
      "metadata": {
        "id": "vEC1AeKd9Igr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class AggregateFeatureTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Creates customer-level aggregate features\"\"\"\n",
        "\n",
        "    def __init__(self):\n",
        "        self.customer_stats = {}\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        df = X.copy()\n",
        "\n",
        "        # Customer transaction aggregates\n",
        "        self.customer_stats = df.groupby('CustomerId').agg({\n",
        "            'Amount': ['sum', 'mean', 'std', 'count', 'min', 'max'],\n",
        "            'Value': ['sum', 'mean', 'std'],\n",
        "            'FraudResult': ['sum', 'mean']  # fraud count and rate\n",
        "        }).reset_index()\n",
        "\n",
        "        # Flatten column names\n",
        "        self.customer_stats.columns = ['CustomerId'] + [\n",
        "            f'customer_{col[0]}_{col[1]}' for col in self.customer_stats.columns[1:]\n",
        "        ]\n",
        "\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "\n",
        "        # Merge aggregate features\n",
        "        df_merged = df.merge(self.customer_stats, on='CustomerId', how='left')\n",
        "\n",
        "        # Fill NaN for new customers\n",
        "        agg_cols = [col for col in df_merged.columns if col.startswith('customer_')]\n",
        "        df_merged[agg_cols] = df_merged[agg_cols].fillna(0)\n",
        "\n",
        "        return df_merged\n",
        "\n",
        "class TimeFeatureExtractor(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Extracts time-based features from TransactionStartTime\"\"\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "\n",
        "        # Convert to datetime\n",
        "        df['TransactionStartTime'] = pd.to_datetime(df['TransactionStartTime'])\n",
        "\n",
        "        # Extract time features\n",
        "        df['transaction_hour'] = df['TransactionStartTime'].dt.hour\n",
        "        df['transaction_day'] = df['TransactionStartTime'].dt.day\n",
        "        df['transaction_month'] = df['TransactionStartTime'].dt.month\n",
        "        df['transaction_dayofweek'] = df['TransactionStartTime'].dt.dayofweek\n",
        "\n",
        "        # Business hours (9-17)\n",
        "        df['is_business_hours'] = ((df['transaction_hour'] >= 9) &\n",
        "                                 (df['transaction_hour'] <= 17)).astype(int)\n",
        "\n",
        "        # Weekend\n",
        "        df['is_weekend'] = (df['transaction_dayofweek'].isin([5, 6])).astype(int)\n",
        "\n",
        "        return df\n",
        "\n",
        "class AmountTransformer(BaseEstimator, TransformerMixin):\n",
        "    \"\"\"Handles Amount and Value transformation\"\"\"\n",
        "\n",
        "    def fit(self, X, y=None):\n",
        "        return self\n",
        "\n",
        "    def transform(self, X):\n",
        "        df = X.copy()\n",
        "\n",
        "        # Handle negative amounts (refunds/chargebacks)\n",
        "        df['is_refund'] = (df['Amount'] < 0).astype(int)\n",
        "        df['abs_amount'] = df['Amount'].abs()\n",
        "\n",
        "        # Log transform for skewness (add 1 to handle zeros)\n",
        "        df['amount_log'] = np.log1p(df['abs_amount'])\n",
        "        df['value_log'] = np.log1p(df['Value'])\n",
        "\n",
        "        # Amount bins\n",
        "        df['amount_category'] = pd.cut(df['abs_amount'],\n",
        "                                     bins=[0, 100, 1000, 5000, float('inf')],\n",
        "                                     labels=['small', 'medium', 'large', 'xlarge'])\n",
        "\n",
        "        return df"
      ],
      "metadata": {
        "id": "fSJJ_DoU9GKf"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Main Feature Engineering Pipeline"
      ],
      "metadata": {
        "id": "mZMG7KHq9NAb"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def create_feature_pipeline():\n",
        "    \"\"\"Creates the complete feature engineering pipeline\"\"\"\n",
        "\n",
        "    # Define column groups\n",
        "    categorical_cols = ['ProductCategory', 'ChannelId', 'PricingStrategy']\n",
        "    high_cardinality_cols = ['ProviderId', 'ProductId']\n",
        "    numerical_cols = ['Amount', 'Value']\n",
        "\n",
        "    # Custom transformers pipeline\n",
        "    custom_pipeline = Pipeline([\n",
        "        ('aggregate', AggregateFeatureTransformer()),\n",
        "        ('time_features', TimeFeatureExtractor()),\n",
        "        ('amount_transform', AmountTransformer())\n",
        "    ])\n",
        "\n",
        "    # Preprocessing for different column types\n",
        "    preprocessor = ColumnTransformer([\n",
        "        # Categorical - One Hot Encoding\n",
        "        ('cat_onehot', OneHotEncoder(drop='first', sparse_output=False),\n",
        "         categorical_cols),\n",
        "\n",
        "        # High cardinality - Imputation + Ordinal Encoding\n",
        "        ('high_card', Pipeline([\n",
        "            ('imputer', SimpleImputer(strategy='most_frequent')),\n",
        "            ('ordinal_enc', OrdinalEncoder(handle_unknown='use_encoded_value', unknown_value=-1)) # Use OrdinalEncoder here\n",
        "        ]), high_cardinality_cols),\n",
        "\n",
        "        # Numerical - Imputation + Scaling\n",
        "        ('numerical', Pipeline([\n",
        "            ('imputer', KNNImputer(n_neighbors=5)),\n",
        "            ('scaler', StandardScaler())\n",
        "        ]), numerical_cols + ['amount_log', 'value_log', 'abs_amount']),\n",
        "\n",
        "        # Time features - Min-Max scaling\n",
        "        ('time', MinMaxScaler(),\n",
        "         ['transaction_hour', 'transaction_day', 'transaction_month'])\n",
        "    ], remainder='passthrough')\n",
        "\n",
        "    # Complete pipeline\n",
        "    feature_pipeline = Pipeline([\n",
        "        ('custom_features', custom_pipeline),\n",
        "        ('preprocessing', preprocessor)\n",
        "    ])\n",
        "\n",
        "    return feature_pipeline"
      ],
      "metadata": {
        "id": "-A2ON9OG9SE3"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Execute Feature Engineering"
      ],
      "metadata": {
        "id": "EZgjQfqV9bLD"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create and fit the pipeline\n",
        "print(\"Creating feature engineering pipeline...\")\n",
        "pipeline = create_feature_pipeline()\n",
        "\n",
        "# Fit and transform the data\n",
        "print(\"Fitting pipeline on training data...\")\n",
        "X_transformed = pipeline.fit_transform(df)\n",
        "\n",
        "print(f\"Original shape: {df.shape}\")\n",
        "print(f\"Transformed shape: {X_transformed.shape}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GIwFWEzD9gkC",
        "outputId": "33940a82-7ec4-4d7a-8bec-451bb0d6c38c"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Creating feature engineering pipeline...\n",
            "Fitting pipeline on training data...\n",
            "Original shape: (95662, 14)\n",
            "Transformed shape: (95662, 47)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Feature Analysis\n"
      ],
      "metadata": {
        "id": "73aDpAVo9k4Z"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get feature names after transformation\n",
        "def get_feature_names(pipeline, df):\n",
        "    \"\"\"Extract feature names from the fitted pipeline\"\"\"\n",
        "\n",
        "    # Apply custom transformations first\n",
        "    df_custom = pipeline.named_steps['custom_features'].transform(df)\n",
        "\n",
        "    # Get preprocessor\n",
        "    preprocessor = pipeline.named_steps['preprocessing']\n",
        "    feature_names = []\n",
        "\n",
        "    for name, transformer, columns in preprocessor.transformers_:\n",
        "        if name == 'cat_onehot':\n",
        "            if hasattr(transformer, 'get_feature_names_out'):\n",
        "                names = transformer.get_feature_names_out(columns).tolist()\n",
        "            else:\n",
        "                names = [f\"{col}_{val}\" for col in columns for val in transformer.categories_[columns.index(col)][1:]]\n",
        "        elif name == 'high_card':\n",
        "            names = [f\"{col}_encoded\" for col in columns]\n",
        "        elif name == 'numerical':\n",
        "            names = columns\n",
        "        elif name == 'time':\n",
        "            names = columns\n",
        "        elif name == 'remainder':\n",
        "            # Handle remainder columns\n",
        "            remainder_cols = [col for col in df_custom.columns\n",
        "                            if col not in sum([list(cols) for _, _, cols in preprocessor.transformers_[:-1]], [])]\n",
        "            names = remainder_cols\n",
        "        else:\n",
        "            names = columns if isinstance(columns, list) else [columns]\n",
        "\n",
        "        feature_names.extend(names)\n",
        "\n",
        "    return feature_names\n",
        "\n",
        "# Get feature names and create DataFrame\n",
        "try:\n",
        "    feature_names = get_feature_names(pipeline, df)\n",
        "    X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)\n",
        "\n",
        "    print(\"Feature engineering completed!\")\n",
        "    print(f\"Total features created: {len(feature_names)}\")\n",
        "\n",
        "    # Count feature types\n",
        "    cat_count = sum(1 for name in feature_names if any(cat in name for cat in ['ProductCategory', 'ChannelId', 'PricingStrategy']))\n",
        "    high_card_count = sum(1 for name in feature_names if 'encoded' in name)\n",
        "    numerical_count = sum(1 for name in feature_names if name in ['Amount', 'Value', 'amount_log', 'value_log', 'abs_amount'])\n",
        "    time_count = sum(1 for name in feature_names if 'transaction_' in name or name in ['is_business_hours', 'is_weekend'])\n",
        "    customer_count = sum(1 for name in feature_names if 'customer_' in name)\n",
        "\n",
        "    print(\"\\nFeature types:\")\n",
        "    print(f\"- categorical_onehot: {cat_count}\")\n",
        "    print(f\"- high_cardinality_encoded: {high_card_count}\")\n",
        "    print(f\"- numerical: {numerical_count}\")\n",
        "    print(f\"- time_feature: {time_count}\")\n",
        "    print(f\"- remainder: {len(feature_names) - cat_count - high_card_count - numerical_count - time_count}\")\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Error getting feature names: {e}\")\n",
        "    feature_names = [f\"feature_{i}\" for i in range(X_transformed.shape[1])]\n",
        "    X_transformed_df = pd.DataFrame(X_transformed, columns=feature_names)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VnWjaYPO9oMf",
        "outputId": "7c61a4a3-4af2-4b04-e5e4-ba26b0f3bf95"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Feature engineering completed!\n",
            "Total features created: 47\n",
            "\n",
            "Feature types:\n",
            "- categorical_onehot: 14\n",
            "- high_cardinality_encoded: 2\n",
            "- numerical: 5\n",
            "- time_feature: 6\n",
            "- remainder: 20\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Class Imbalance Analysis\n"
      ],
      "metadata": {
        "id": "XG9eAo1p9xHe"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Check class imbalance for fraud detection\n",
        "print(\"CLASS IMBALANCE ANALYSIS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "fraud_counts = df['FraudResult'].value_counts()\n",
        "fraud_percentages = df['FraudResult'].value_counts(normalize=True) * 100\n",
        "\n",
        "print(\"Fraud Distribution:\")\n",
        "for class_val, count in fraud_counts.items():\n",
        "    percentage = fraud_percentages[class_val]\n",
        "    print(f\"Class {class_val}: {count:,} samples ({percentage:.2f}%)\")\n",
        "\n",
        "# Calculate imbalance ratio\n",
        "fraud_ratio = fraud_counts[0] / fraud_counts[1] if len(fraud_counts) > 1 else 0\n",
        "print(f\"Imbalance Ratio: {fraud_ratio:.1f}:1 (non-fraud:fraud)\")\n",
        "\n",
        "if fraud_ratio > 10:\n",
        "    print(\"⚠️  SEVERE CLASS IMBALANCE DETECTED!\")\n",
        "    print(\"Recommendations:\")\n",
        "    print(\"- Use SMOTE or ADASYN for oversampling\")\n",
        "    print(\"- Apply class weights in model\")\n",
        "    print(\"- Consider ensemble methods (Random Forest, XGBoost)\")\n",
        "    print(\"- Use stratified sampling\")\n",
        "    print(\"- Focus on Precision, Recall, F1-score over Accuracy\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pXOkCZAH901q",
        "outputId": "c40a4a6e-f4b6-44b4-afc4-aeecdf0fb7d7"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "CLASS IMBALANCE ANALYSIS\n",
            "========================================\n",
            "Fraud Distribution:\n",
            "Class 0: 95,469 samples (99.80%)\n",
            "Class 1: 193 samples (0.20%)\n",
            "Imbalance Ratio: 494.7:1 (non-fraud:fraud)\n",
            "⚠️  SEVERE CLASS IMBALANCE DETECTED!\n",
            "Recommendations:\n",
            "- Use SMOTE or ADASYN for oversampling\n",
            "- Apply class weights in model\n",
            "- Consider ensemble methods (Random Forest, XGBoost)\n",
            "- Use stratified sampling\n",
            "- Focus on Precision, Recall, F1-score over Accuracy\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "8. Skewness Analysis"
      ],
      "metadata": {
        "id": "sy8dE_Gk96NW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Analyze skewness before and after transformation\n",
        "from scipy import stats\n",
        "\n",
        "print(\"SKEWNESS ANALYSIS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "# Original skewness\n",
        "original_amount_skew = stats.skew(df['Amount'])\n",
        "original_value_skew = stats.skew(df['Value'])\n",
        "\n",
        "print(\"Original Skewness:\")\n",
        "print(f\"Amount: {original_amount_skew:.2f}\")\n",
        "print(f\"Value: {original_value_skew:.2f}\")\n",
        "\n",
        "# After log transformation\n",
        "df_with_logs = pipeline.named_steps['custom_features'].transform(df)\n",
        "log_amount_skew = stats.skew(df_with_logs['amount_log'])\n",
        "log_value_skew = stats.skew(df_with_logs['value_log'])\n",
        "\n",
        "print(\"After Log Transform:\")\n",
        "print(f\"Amount (log): {log_amount_skew:.2f}\")\n",
        "print(f\"Value (log): {log_value_skew:.2f}\")\n",
        "\n",
        "print(\"Skewness Reduction:\")\n",
        "print(f\"Amount: {original_amount_skew:.2f} → {log_amount_skew:.2f} ({abs(original_amount_skew - log_amount_skew):.1f} improvement)\")\n",
        "print(f\"Value: {original_value_skew:.2f} → {log_value_skew:.2f} ({abs(original_value_skew - log_value_skew):.1f} improvement)\")\n",
        "\n",
        "if abs(log_amount_skew) < 2 and abs(log_value_skew) < 2:\n",
        "    print(\"✅ Log transformation successfully reduced skewness!\")\n",
        "else:\n",
        "    print(\"⚠️  Consider additional transformations (Box-Cox, Yeo-Johnson)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "pybUKaGx9-QU",
        "outputId": "b165725f-5d7f-47e7-b40f-e5af4fad2edd"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "SKEWNESS ANALYSIS\n",
            "========================================\n",
            "Original Skewness:\n",
            "Amount: 51.10\n",
            "Value: 51.29\n",
            "After Log Transform:\n",
            "Amount (log): -0.21\n",
            "Value (log): -0.20\n",
            "Skewness Reduction:\n",
            "Amount: 51.10 → -0.21 (51.3 improvement)\n",
            "Value: 51.29 → -0.20 (51.5 improvement)\n",
            "✅ Log transformation successfully reduced skewness!\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/pipeline.py:62: FutureWarning: This Pipeline instance is not fitted yet. Call 'fit' with appropriate arguments before using other methods such as transform, predict, etc. This will raise an error in 1.8 instead of the current warning.\n",
            "  warnings.warn(\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "9. Missing Values Analysis\n"
      ],
      "metadata": {
        "id": "-_u0ICr4-IWO"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MISSING VALUES ANALYSIS\")\n",
        "print(\"=\"*40)\n",
        "\n",
        "print(\"Original missing values:\")\n",
        "original_missing = df.isnull().sum()\n",
        "print(original_missing[original_missing > 0] if original_missing.sum() > 0 else \"No missing values\")\n",
        "\n",
        "print(\"Missing values in transformed data:\")\n",
        "# Select only numeric columns from the transformed DataFrame for NaN check\n",
        "numeric_cols_transformed = X_transformed_df.select_dtypes(include=np.number)\n",
        "\n",
        "if numeric_cols_transformed.isnull().values.any():\n",
        "    missing_counts = numeric_cols_transformed.isnull().sum()\n",
        "    print(missing_counts[missing_counts > 0])\n",
        "else:\n",
        "    print(\"✅ No missing values in final transformed data!\")\n",
        "\n",
        "print(\"Imputation Summary:\")\n",
        "print(\"- KNN Imputer used for numerical features\")\n",
        "print(\"- Most frequent strategy for categorical features\")\n",
        "print(\"- Customer aggregates filled with 0 for new customers\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "nVdTSYC0-JRu",
        "outputId": "5fb4ab3d-17f6-41ee-bba1-24ee7f8e28ad"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MISSING VALUES ANALYSIS\n",
            "========================================\n",
            "Original missing values:\n",
            "No missing values\n",
            "Missing values in transformed data:\n",
            "✅ No missing values in final transformed data!\n",
            "Imputation Summary:\n",
            "- KNN Imputer used for numerical features\n",
            "- Most frequent strategy for categorical features\n",
            "- Customer aggregates filled with 0 for new customers\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "10. Save Pipeline and Data"
      ],
      "metadata": {
        "id": "zS5YO4uA-PWa"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Create output directories\n",
        "os.makedirs('models/', exist_ok=True)\n",
        "os.makedirs('data/processed/', exist_ok=True)\n",
        "\n",
        "# Save the fitted pipeline\n",
        "joblib.dump(pipeline, 'models/feature_pipeline.pkl')\n",
        "\n",
        "# Save transformed data\n",
        "X_transformed_df.to_csv('data/processed/features_engineered.csv', index=False)\n",
        "\n",
        "# Save feature names\n",
        "with open('data/processed/feature_names.txt', 'w') as f:\n",
        "    for name in feature_names:\n",
        "        f.write(f\"{name}\\n\")\n",
        "\n",
        "print(\"FEATURE ENGINEERING PIPELINE SUMMARY\")\n",
        "print(\"=\"*60)\n",
        "print(f\"✓ Data processed: {df.shape[0]:,} rows\")\n",
        "print(f\"✓ Features created: {X_transformed.shape[1]} total\")\n",
        "print(f\"✓ Custom transformers: 3 (Aggregates, Time, Amount)\")\n",
        "print(f\"✓ Encoding: One-hot + Label encoding\")\n",
        "print(f\"✓ Imputation: KNN for numerical, Mode for categorical\")\n",
        "print(f\"✓ Scaling: StandardScaler + MinMaxScaler\")\n",
        "print(f\"✓ Pipeline saved: models/feature_pipeline.pkl\")\n",
        "print(f\"✓ Data saved: data/processed/features_engineered.csv\")\n",
        "print(\"=\"*60)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FPupsiI0-Tqu",
        "outputId": "0d16cf6a-50d6-435a-9484-46964ebb99da"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "FEATURE ENGINEERING PIPELINE SUMMARY\n",
            "============================================================\n",
            "✓ Data processed: 95,662 rows\n",
            "✓ Features created: 47 total\n",
            "✓ Custom transformers: 3 (Aggregates, Time, Amount)\n",
            "✓ Encoding: One-hot + Label encoding\n",
            "✓ Imputation: KNN for numerical, Mode for categorical\n",
            "✓ Scaling: StandardScaler + MinMaxScaler\n",
            "✓ Pipeline saved: models/feature_pipeline.pkl\n",
            "✓ Data saved: data/processed/features_engineered.csv\n",
            "============================================================\n"
          ]
        }
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}